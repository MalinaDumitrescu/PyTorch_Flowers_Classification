{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = \"data/flower_photos\"\n",
    "image_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "default_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Default device:\", default_device)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_dataloaders():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=image_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # dataset de bază doar pentru split\n",
    "    base_dataset = datasets.ImageFolder(root=data_dir)\n",
    "    class_names = base_dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    print(\"Classes:\", class_names)\n",
    "    print(\"Number of images:\", len(base_dataset))\n",
    "\n",
    "    train_ratio = 0.8\n",
    "    train_size = int(train_ratio * len(base_dataset))\n",
    "    val_size = len(base_dataset) - train_size\n",
    "\n",
    "    train_subset_base, val_subset_base = random_split(\n",
    "        base_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    train_indices = train_subset_base.indices\n",
    "    val_indices = val_subset_base.indices\n",
    "\n",
    "    full_train_dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\n",
    "    full_val_dataset = datasets.ImageFolder(root=data_dir, transform=val_transform)\n",
    "\n",
    "    train_dataset = Subset(full_train_dataset, train_indices)\n",
    "    val_dataset = Subset(full_val_dataset, val_indices)\n",
    "\n",
    "    print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    print(\"Number of train batches:\", len(train_loader))\n",
    "    print(\"Number of val batches:\", len(val_loader))\n",
    "\n",
    "    return train_loader, val_loader, class_names, num_classes\n",
    "\n",
    "\n",
    "train_loader, val_loader, class_names, num_classes = get_dataloaders()\n"
   ],
   "id": "38a948129bbca8cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_activation(name: str):\n",
    "    name = name.lower()\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    if name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    if name == \"leakyrelu\":\n",
    "        return nn.LeakyReLU(0.1)\n",
    "    raise ValueError(f\"Unknown activation {name}\")\n",
    "\n",
    "\n",
    "class FlexibleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Model flexibil:\n",
    "    - 0–4 convolutional layers\n",
    "    - opțional BatchNorm\n",
    "    - opțional MaxPooling\n",
    "    - 1–2 fully connected (hidden) layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        num_conv_layers: int = 3,\n",
    "        num_hidden_layers: int = 1,\n",
    "        hidden_dim: int = 256,\n",
    "        use_batchnorm: bool = True,\n",
    "        use_pooling: bool = True,\n",
    "        activation: str = \"relu\",\n",
    "        dropout_p: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_pooling = use_pooling\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        # ---- features (conv blocks) ----\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        out_channels = 32\n",
    "\n",
    "        if num_conv_layers > 0:\n",
    "            for i in range(num_conv_layers):\n",
    "                layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "                if use_batchnorm:\n",
    "                    layers.append(nn.BatchNorm2d(out_channels))\n",
    "                layers.append(self.activation)\n",
    "                if use_pooling:\n",
    "                    layers.append(nn.MaxPool2d(2))\n",
    "\n",
    "                in_channels = out_channels\n",
    "                out_channels *= 2\n",
    "\n",
    "            layers.append(nn.AdaptiveAvgPool2d((4, 4)))\n",
    "            self.features = nn.Sequential(*layers)\n",
    "\n",
    "            last_channels = in_channels\n",
    "            fc_in_dim = last_channels * 4 * 4\n",
    "        else:\n",
    "            self.features = nn.Identity()\n",
    "            fc_in_dim = 3 * image_size * image_size\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # ---- classifier (fully connected) ----\n",
    "        fc_layers = []\n",
    "        if num_hidden_layers == 1:\n",
    "            fc_layers.append(nn.Linear(fc_in_dim, hidden_dim))\n",
    "            fc_layers.append(self.activation)\n",
    "            fc_layers.append(nn.Dropout(dropout_p))\n",
    "            fc_layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "        elif num_hidden_layers == 2:\n",
    "            fc_layers.append(nn.Linear(fc_in_dim, hidden_dim))\n",
    "            fc_layers.append(self.activation)\n",
    "            fc_layers.append(nn.Dropout(dropout_p))\n",
    "            fc_layers.append(nn.Linear(hidden_dim, hidden_dim // 2))\n",
    "            fc_layers.append(self.activation)\n",
    "            fc_layers.append(nn.Dropout(dropout_p))\n",
    "            fc_layers.append(nn.Linear(hidden_dim // 2, num_classes))\n",
    "        else:\n",
    "            raise ValueError(\"num_hidden_layers must be 1 or 2\")\n",
    "\n",
    "        self.classifier = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# sanity check\n",
    "test_model = FlexibleCNN(num_classes=num_classes, num_conv_layers=2, num_hidden_layers=1)\n",
    "print(test_model)\n"
   ],
   "id": "6fb1af38825296da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, l1_lambda: float = 0.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if l1_lambda > 0.0:\n",
    "            l1_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                l1_norm += p.abs().sum()\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "    epoch_time = time.time() - start_time\n",
    "    return epoch_loss, epoch_acc, epoch_time\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            running_total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_labels, all_preds\n"
   ],
   "id": "a12dcebb9766a041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_history(history, title_prefix=\"\"):\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{title_prefix} - Loss per Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{title_prefix} - Accuracy per Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion(all_labels, all_preds, class_names, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    print(\"\\n Model parameters (weights & biases)\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: shape={tuple(param.shape)}\")\n"
   ],
   "id": "db766a5e8c56574b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    name: str\n",
    "    num_conv_layers: int\n",
    "    num_hidden_layers: int\n",
    "    hidden_dim: int\n",
    "    use_batchnorm: bool\n",
    "    use_pooling: bool\n",
    "    activation: str\n",
    "    learning_rate: float\n",
    "    l2_lambda: float\n",
    "    l1_lambda: float\n",
    "    num_epochs: int\n",
    "    patience: int = 5\n",
    "    label_smoothing: float = 0.1\n",
    "    use_scheduler: bool = False\n",
    "    use_class_weights: bool = False   # <--- nou\n",
    "\n",
    "\n",
    "def get_experiments() -> List[ExperimentConfig]:\n",
    "    # aici pun doar modelul 14; dacă vrei, poți adăuga și celelalte 0–12 la fel ca înainte\n",
    "    return [\n",
    "        ExperimentConfig(\n",
    "            name=\"14\",\n",
    "            num_conv_layers=4,\n",
    "            num_hidden_layers=2,\n",
    "            hidden_dim=640,\n",
    "            use_batchnorm=True,\n",
    "            use_pooling=True,\n",
    "            activation=\"relu\",\n",
    "            learning_rate=1.6e-4,\n",
    "            l2_lambda=5e-4,\n",
    "            l1_lambda=0.0,\n",
    "            num_epochs=50,\n",
    "            patience=8,\n",
    "            label_smoothing=0.05,\n",
    "            use_scheduler=True,\n",
    "            use_class_weights=True,   # <--- activat\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "experiments = get_experiments()\n",
    "print(experiments[0])\n"
   ],
   "id": "691f798cc707bc2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_experiment(\n",
    "    cfg: ExperimentConfig,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    class_names: List[str],\n",
    "    num_classes: int,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, Any]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Starting experiment: {cfg.name} on device {device}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    model = FlexibleCNN(\n",
    "        num_classes=num_classes,\n",
    "        num_conv_layers=cfg.num_conv_layers,\n",
    "        num_hidden_layers=cfg.num_hidden_layers,\n",
    "        hidden_dim=cfg.hidden_dim,\n",
    "        use_batchnorm=cfg.use_batchnorm,\n",
    "        use_pooling=cfg.use_pooling,\n",
    "        activation=cfg.activation,\n",
    "        dropout_p=0.3,  # ca la modelul tău 14\n",
    "    ).to(device)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    # -------- class weights pentru macro-F1 mai bun --------\n",
    "    class_weights = None\n",
    "    if cfg.use_class_weights:\n",
    "        # ordine: ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "        class_counts = torch.tensor([633., 898., 641., 699., 799.])\n",
    "        class_weights = class_counts.sum() / (len(class_counts) * class_counts)\n",
    "        class_weights = class_weights.to(device)\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "\n",
    "    if class_weights is not None:\n",
    "        criterion = nn.CrossEntropyLoss(\n",
    "            label_smoothing=cfg.label_smoothing,\n",
    "            weight=class_weights,\n",
    "        )\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=cfg.learning_rate,\n",
    "        weight_decay=cfg.l2_lambda,\n",
    "    )\n",
    "\n",
    "    # scheduler (CosineAnnealingLR) – folosit la modelul 14\n",
    "    if cfg.use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.num_epochs,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"epoch_time\": [],\n",
    "    }\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_acc = 0.0\n",
    "    best_state_dict = None\n",
    "    best_epoch = 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    for epoch in range(1, cfg.num_epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{cfg.num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        train_loss, train_acc, train_time = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, l1_lambda=cfg.l1_lambda\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"epoch_time\"].append(train_time)\n",
    "\n",
    "        print(f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}\")\n",
    "        print(f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
    "        print(f\"Epoch train time: {train_time:.1f} s\")\n",
    "\n",
    "        # update best\n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            best_state_dict = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            no_improve_epochs = 0\n",
    "            print(f\"new best model (val_loss={best_val_loss:.4f}, val_acc={best_val_acc:.4f})\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"no improvement for {no_improve_epochs} epoch(s)\")\n",
    "\n",
    "        # scheduler step\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # early stopping\n",
    "        if no_improve_epochs >= cfg.patience:\n",
    "            print(f\" early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nTRAINING FINISHED FOR {cfg.name}\")\n",
    "    print(f\"Best epoch: {best_epoch} / {cfg.num_epochs}\")\n",
    "    print(f\"Best val_loss: {best_val_loss:.4f} | Best val_acc: {best_val_acc:.4f}\")\n",
    "    print(f\"Total training time: {total_time:.1f} s\")\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    # Final prediction + macro-F1\n",
    "    all_labels, all_preds = get_predictions(model, val_loader, device)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    print(f\"Macro-F1 on validation: {macro_f1:.4f}\")\n",
    "\n",
    "    print_model_parameters(model)\n",
    "\n",
    "    plot_history(history, title_prefix=cfg.name)\n",
    "    plot_confusion(all_labels, all_preds, class_names, title=f\"{cfg.name} - Confusion Matrix\")\n",
    "\n",
    "    result = {\n",
    "        \"config\": cfg,\n",
    "        \"history\": history,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"total_time\": total_time,\n",
    "    }\n",
    "    return result\n"
   ],
   "id": "47154c8461e587ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cfg14 = experiments[0]          ",
    "result_14 = run_experiment(\n",
    "    cfg14,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    class_names,\n",
    "    num_classes,\n",
    "    default_device,\n",
    ")\n",
    "\n",
    "print(\"Final macro-F1 for 14:\", result_14[\"macro_f1\"])\n"
   ],
   "id": "7394c7e0fa8bf7b2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
